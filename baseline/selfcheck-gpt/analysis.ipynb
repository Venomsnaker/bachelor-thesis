{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "scores_dict = {\n",
    "    \"scores_ngram\": {\n",
    "        \"1-gram\": \"data/scores_ngram/scores_1gram.json\",\n",
    "        \"2-gram\": \"data/scores_ngram/scores_2gram.json\",\n",
    "        \"3-gram\": \"data/scores_ngram/scores_3gram.json\",\n",
    "        \"4-gram\": \"data/scores_ngram/scores_4gram.json\",\n",
    "        \"5-gram\": \"data/scores_ngram/scores_5gram.json\",\n",
    "    },\n",
    "    \"BERTScore\": \"data/scores_bertscore.json\",\n",
    "    \"NLI\": \"data/scores_nli.json\",\n",
    "    \"scores_prompt\": {\n",
    "        \"Llama\": \"data/scores_prompt/scores_llama.json\",\n",
    "        \"Solar Pro\": \"data/scores_prompt/scores_solar_pro.json\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def load_scores(scores_dict):\n",
    "    ngram_scores = {}\n",
    "    raw_scores = {}\n",
    "    \n",
    "    for key in scores_dict:\n",
    "        if key == \"scores_ngram\":\n",
    "            for ngram_key in scores_dict[key]:\n",
    "                with open(scores_dict[key][ngram_key]) as file:\n",
    "                    ngram_scores[ngram_key] = json.load(file)\n",
    "        elif key == \"scores_prompt\":\n",
    "            for prompt_key in scores_dict[key]:\n",
    "                with open(scores_dict[key][prompt_key]) as file:\n",
    "                    raw_scores[prompt_key] = json.load(file)\n",
    "        else:\n",
    "            with open(scores_dict[key]) as file:\n",
    "                raw_scores[key] = json.load(file)\n",
    "    raw_scores['scores_ngram'] = ngram_scores\n",
    "    return raw_scores\n",
    "    \n",
    "raw_scores = load_scores(scores_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def map_ngram_scores(value):\n",
    "    if isinstance(value, list):\n",
    "        return [1 / (1 + math.exp(-x)) if x != float('inf') else 1.0 for x in value]\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-value)) if value != float('inf') else 1.0\n",
    "\n",
    "def process_ngram_scores(ngram_name, scores):\n",
    "    scores_sent_avg = {}\n",
    "    scores_sent_max = {}\n",
    "    scores_doc_avg = {}\n",
    "    scores_doc_avg_max = {}\n",
    "    \n",
    "    for idx in scores:\n",
    "        scores_sent = scores[idx]['sent_level']\n",
    "        scores_doc = scores[idx]['doc_level']\n",
    "        \n",
    "        scores_sent_avg[int(idx)] = map_ngram_scores(scores_sent['avg_neg_logprob'])\n",
    "        scores_sent_max[int(idx)] = map_ngram_scores(scores_sent['max_neg_logprob'])\n",
    "        scores_doc_avg[int(idx)] = map_ngram_scores(scores_doc['avg_neg_logprob'])\n",
    "        scores_doc_avg_max[int(idx)] = map_ngram_scores(scores_doc['avg_max_neg_logprob'])\n",
    "    return {\n",
    "        ngram_name + \" Sent Avg\": scores_sent_avg,\n",
    "        ngram_name + \" Sent Max\": scores_sent_max,\n",
    "        ngram_name + \" Doc Avg\": scores_doc_avg,\n",
    "        ngram_name + \" Doc Avg Max\": scores_doc_avg_max\n",
    "    }\n",
    "    \n",
    "def process_scores(scores):\n",
    "    processed_scores = {}\n",
    "    \n",
    "    for idx in scores:\n",
    "        processed_scores[int(idx)] = scores[idx]\n",
    "    return processed_scores\n",
    "        \n",
    "processed_scores = {}\n",
    "\n",
    "for key in raw_scores:\n",
    "    if key == \"scores_ngram\":\n",
    "        processed_scores[key] = {}\n",
    "        \n",
    "        for ngram_key in raw_scores[key]:\n",
    "            processed_scores[key][ngram_key] = process_ngram_scores(ngram_key, raw_scores[key][ngram_key])\n",
    "    else:\n",
    "        processed_scores[key] = process_scores(raw_scores[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Wikibio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset: 238\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/dataset_v3.json\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    \n",
    "dataset = json.loads(content)\n",
    "print(\"The length of the dataset: {}\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of False: 238\n",
      "Length of True: 238\n",
      "Length of False_h: 206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_mapping = {\n",
    "    'accurate': 0.0,\n",
    "    'minor_inaccurate': 0.5,\n",
    "    'major_inaccurate': 1.0,\n",
    "}\n",
    "\n",
    "indices = [x['wiki_bio_test_idx'] for x in dataset] \n",
    "human_label_detect_False   = {}\n",
    "human_label_detect_True    = {}\n",
    "human_label_detect_False_h = {}\n",
    "human_label_raw = {}\n",
    "\n",
    "for i_ in range(len(dataset)):\n",
    "    dataset_i = dataset[i_]\n",
    "    idx = dataset_i[\"wiki_bio_test_idx\"]\n",
    "    raw_label = np.array([label_mapping[x] for x in dataset_i['annotation']])\n",
    "    \n",
    "    human_label_raw[idx] = raw_label\n",
    "    human_label_detect_False[idx] = (raw_label > 0.499).astype(np.int32).tolist()\n",
    "    human_label_detect_True[idx] = (raw_label < 0.499).astype(np.int32).tolist()\n",
    "    average_score = np.mean(raw_label)\n",
    "    if (average_score < 0.99):\n",
    "        human_label_detect_False_h[idx] = (raw_label > 0.99).astype(np.int32).tolist()\n",
    "        \n",
    "print(\"Length of False:\", len(human_label_detect_False))\n",
    "print(\"Length of True:\", len(human_label_detect_True)) \n",
    "print(\"Length of False_h:\", len(human_label_detect_False_h))\n",
    "\n",
    "human_label_passage_avg = []\n",
    "\n",
    "for id in indices:\n",
    "    human_label_passage_avg.append(np.mean(human_label_raw[id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline false: 0.73\n",
      "Random baseline false h: 0.3\n",
      "Random baseline true: 0.27\n"
     ]
    }
   ],
   "source": [
    "arr_false = []\n",
    "arr_false_h = []\n",
    "arr_true = []\n",
    "\n",
    "for v in human_label_detect_False.values():\n",
    "    arr_false.extend(v)\n",
    "for v in human_label_detect_False_h.values():\n",
    "    arr_false_h.extend(v)\n",
    "for v in human_label_detect_True.values():\n",
    "    arr_true.extend(v)\n",
    "    \n",
    "random_baseline_false = np.mean(arr_false)\n",
    "random_baseline_false_h = np.mean(arr_false_h)\n",
    "random_baseline_true = np.mean(arr_true)\n",
    "\n",
    "print(\"Random baseline false:\", np.round(random_baseline_false, 2))\n",
    "print(\"Random baseline false h:\", np.round(random_baseline_false_h, 2))\n",
    "print(\"Random baseline true:\", np.round(random_baseline_true, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def unroll_pred(scores, indices):\n",
    "    unrolled = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        unrolled.extend(scores[idx])\n",
    "    return unrolled\n",
    "\n",
    "def get_PR_with_human_labels(preds, human_labels, pos_label=1, oneminus_pred=False):\n",
    "    indices = [k for k in human_labels.keys()]\n",
    "    \n",
    "    unroll_preds = unroll_pred(preds, indices)\n",
    "    if oneminus_pred:\n",
    "        unroll_preds = [1.0-x for x in unroll_preds]\n",
    "        \n",
    "    unroll_labels = unroll_pred(human_labels, indices)\n",
    "    assert(len(unroll_preds) == len(unroll_labels))\n",
    "    \n",
    "    p, r, threshold = precision_recall_curve(unroll_labels, unroll_preds, pos_label=pos_label)\n",
    "    return p, r, threshold\n",
    "\n",
    "def get_AUC(p, r):\n",
    "    return (auc(r, p) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_accuracy_score(scores, labels, oneminus_pred=False):\n",
    "    prec, rec, threshold = get_PR_with_human_labels(scores, labels, pos_label=1, oneminus_pred=oneminus_pred)\n",
    "    return np.round(get_AUC(prec, rec), 2)\n",
    "\n",
    "def get_passage_scores(scores):\n",
    "    passage_scores = []\n",
    "    \n",
    "    for id in indices:\n",
    "        passage_scores.append(np.mean(scores[id]))\n",
    "    \n",
    "    score_pearsonnr = stats.pearsonr(passage_scores, human_label_passage_avg)\n",
    "    score_spearmanr = stats.spearmanr(passage_scores, human_label_passage_avg)\n",
    "    return np.round(score_pearsonnr[0] * 100, 2), np.round(score_spearmanr[0] * 100, 2)\n",
    "\n",
    "def get_baseline_row(name, scores):\n",
    "    passage_scores = get_passage_scores(scores)\n",
    "\n",
    "    row = {\n",
    "        \"Method\": name,\n",
    "        \"NonFact\": get_accuracy_score(scores, human_label_detect_False),\n",
    "        \"NonFact-H\": get_accuracy_score(scores, human_label_detect_False_h),\n",
    "        \"Factual\": get_accuracy_score(scores, human_label_detect_True, oneminus_pred=True),\n",
    "        \"Pearson\": passage_scores[0],\n",
    "        \"Spearman\": passage_scores[1]\n",
    "    }\n",
    "    return list(row.values())\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Method\", \"NonFact\", \"NonFact-H\", \"Factual\", \"Pearson\", \"Spearman\"])\n",
    "random_baseline = {\n",
    "        \"Method\": \"Random Baseline\",\n",
    "        \"NonFact\": np.round(random_baseline_false * 100, 2),\n",
    "        \"NonFact-H\": np.round(random_baseline_false_h * 100, 2),\n",
    "        \"Factual\": np.round(random_baseline_true * 100, 2),\n",
    "        \"Pearson\": \"NULL\",\n",
    "        \"Spearman\": \"NULL\",\n",
    "    }\n",
    "df.loc[len(df)] = list(random_baseline.values())\n",
    "\n",
    "for key in processed_scores:\n",
    "    if key == \"scores_ngram\":\n",
    "        for ngram_key in processed_scores[key]:\n",
    "            # Get the first two entry of ngram scores\n",
    "            i = 0\n",
    "            \n",
    "            for ngram_category_key in processed_scores[key][ngram_key]:\n",
    "                if (i > 1): \n",
    "                    break\n",
    "                df.loc[len(df)] = get_baseline_row(ngram_category_key, processed_scores[key][ngram_key][ngram_category_key])\n",
    "                i += 1\n",
    "    else:\n",
    "        df.loc[len(df)] = get_baseline_row(key, processed_scores[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>NonFact</th>\n",
       "      <th>NonFact-H</th>\n",
       "      <th>Factual</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Baseline</td>\n",
       "      <td>72.96</td>\n",
       "      <td>29.72</td>\n",
       "      <td>27.04</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTScore</td>\n",
       "      <td>81.24</td>\n",
       "      <td>45.51</td>\n",
       "      <td>43.41</td>\n",
       "      <td>57.33</td>\n",
       "      <td>54.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NLI</td>\n",
       "      <td>92.50</td>\n",
       "      <td>45.17</td>\n",
       "      <td>66.08</td>\n",
       "      <td>74.14</td>\n",
       "      <td>73.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama 3.2 1B</td>\n",
       "      <td>61.87</td>\n",
       "      <td>23.40</td>\n",
       "      <td>19.61</td>\n",
       "      <td>-40.54</td>\n",
       "      <td>-38.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solar Pro</td>\n",
       "      <td>93.53</td>\n",
       "      <td>54.94</td>\n",
       "      <td>69.35</td>\n",
       "      <td>77.69</td>\n",
       "      <td>76.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-gram Sent Avg</td>\n",
       "      <td>81.52</td>\n",
       "      <td>40.33</td>\n",
       "      <td>41.78</td>\n",
       "      <td>39.2</td>\n",
       "      <td>39.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1-gram Sent Max</td>\n",
       "      <td>85.64</td>\n",
       "      <td>41.05</td>\n",
       "      <td>58.44</td>\n",
       "      <td>63.15</td>\n",
       "      <td>66.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2-gram Sent Avg</td>\n",
       "      <td>82.83</td>\n",
       "      <td>44.11</td>\n",
       "      <td>52.67</td>\n",
       "      <td>59.14</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2-gram Sent Max</td>\n",
       "      <td>85.04</td>\n",
       "      <td>38.97</td>\n",
       "      <td>58.11</td>\n",
       "      <td>56.53</td>\n",
       "      <td>66.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3-gram Sent Avg</td>\n",
       "      <td>83.27</td>\n",
       "      <td>43.79</td>\n",
       "      <td>53.85</td>\n",
       "      <td>59.64</td>\n",
       "      <td>65.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3-gram Sent Max</td>\n",
       "      <td>84.49</td>\n",
       "      <td>36.45</td>\n",
       "      <td>56.95</td>\n",
       "      <td>51.8</td>\n",
       "      <td>60.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4-gram Sent Avg</td>\n",
       "      <td>83.21</td>\n",
       "      <td>42.45</td>\n",
       "      <td>53.89</td>\n",
       "      <td>57.51</td>\n",
       "      <td>63.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4-gram Sent Max</td>\n",
       "      <td>83.71</td>\n",
       "      <td>35.57</td>\n",
       "      <td>55.65</td>\n",
       "      <td>50.37</td>\n",
       "      <td>56.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5-gram Sent Avg</td>\n",
       "      <td>82.85</td>\n",
       "      <td>41.04</td>\n",
       "      <td>53.63</td>\n",
       "      <td>55.92</td>\n",
       "      <td>63.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5-gram Sent Max</td>\n",
       "      <td>83.35</td>\n",
       "      <td>35.40</td>\n",
       "      <td>54.59</td>\n",
       "      <td>49.54</td>\n",
       "      <td>55.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Method  NonFact  NonFact-H  Factual Pearson Spearman\n",
       "0   Random Baseline    72.96      29.72    27.04    NULL     NULL\n",
       "1         BERTScore    81.24      45.51    43.41   57.33    54.54\n",
       "2               NLI    92.50      45.17    66.08   74.14    73.78\n",
       "3      Llama 3.2 1B    61.87      23.40    19.61  -40.54   -38.62\n",
       "4         Solar Pro    93.53      54.94    69.35   77.69    76.42\n",
       "5   1-gram Sent Avg    81.52      40.33    41.78    39.2    39.82\n",
       "6   1-gram Sent Max    85.64      41.05    58.44   63.15    66.09\n",
       "7   2-gram Sent Avg    82.83      44.11    52.67   59.14     62.0\n",
       "8   2-gram Sent Max    85.04      38.97    58.11   56.53    66.21\n",
       "9   3-gram Sent Avg    83.27      43.79    53.85   59.64    65.07\n",
       "10  3-gram Sent Max    84.49      36.45    56.95    51.8    60.56\n",
       "11  4-gram Sent Avg    83.21      42.45    53.89   57.51    63.97\n",
       "12  4-gram Sent Max    83.71      35.57    55.65   50.37    56.82\n",
       "13  5-gram Sent Avg    82.85      41.04    53.63   55.92    63.18\n",
       "14  5-gram Sent Max    83.35      35.40    54.59   49.54    55.02"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
