{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up SelfCheckGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import modeling_selfcheck\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "selfcheck_prompt = modeling_selfcheck.SelfCheckLLMPrompt(llm, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Wikibio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/dataset_v3.json\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    \n",
    "dataset = json.loads(content)\n",
    "print(\"The length of the dataset: {}\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_mapping = {\n",
    "    'accurate': 0.0,\n",
    "    'minor_inaccurate': 0.5,\n",
    "    'major_inaccurate': 1.0,\n",
    "}\n",
    "\n",
    "human_label_detect_False   = {}\n",
    "human_label_detect_True    = {}\n",
    "human_label_detect_False_h = {}\n",
    "\n",
    "for i_ in range(len(dataset)):\n",
    "    dataset_i = dataset[i_]\n",
    "    idx = dataset_i[\"wiki_bio_test_idx\"]\n",
    "    raw_label = np.array([label_mapping[x] for x in dataset_i['annotation']])\n",
    "    \n",
    "    human_label_detect_False[idx] = (raw_label > 0.499).astype(np.int32).tolist()\n",
    "    human_label_detect_True[idx] = (raw_label < 0.499).astype(np.int32).tolist()\n",
    "    average_score = np.mean(raw_label)\n",
    "    if (average_score < 0.99):\n",
    "        human_label_detect_False_h[idx] = (raw_label > 0.99).astype(np.int32).tolist()\n",
    "        \n",
    "print(\"Length of False:\", len(human_label_detect_False))\n",
    "print(\"Length of True:\", len(human_label_detect_True)) \n",
    "print(\"Length of False_h:\", len(human_label_detect_False_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelfCheck Prompt Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "scores_prompt = {}\n",
    "scores_prompt_json = {}\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    x = dataset[i]\n",
    "    idx = dataset[i]['wiki_bio_test_idx'] \n",
    "\n",
    "    scores_prompt[idx] = selfcheck_prompt.predict(\n",
    "        sentences = x['gpt3_sentences'],           \n",
    "        sampled_passages = x['gpt3_text_samples'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "for idx in scores_prompt:\n",
    "  scores = scores_prompt[idx]\n",
    "  scores_prompt_json[idx] = [score for score in scores]\n",
    "  \n",
    "with open(\"data/scores_prompt/scores_llama.json\", \"w\") as outfile:\n",
    "    json.dump(scores_prompt_json, outfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
