{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd067b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ebc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gpt_model import OpenAIClient\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAIClient(api_key=openai_api_key,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ae17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of HaluEval data: 300\n",
      "\n",
      "Reconstruct prompt: \n",
      "Query: You are given an answer. Your task is to generate the original question that was asked to get this answer. Make sure the original question you create explicitly includes the key information provided in the given answer. Only return the original question.\n",
      "\n",
      "Answer: \"{answer}\"\n",
      "\n",
      "Original Question:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data_halu_eval\n",
    "\n",
    "halu_eval_file_path = '../data/interrogate_llm/halu_eval_long_answer.json'\n",
    "output_halu_eval_file_path = '../data/output/interrogate_llm_zeroshot/halu_eval_long_answer_output.json'\n",
    "reconstruct_prompt_file_path = 'prompts/reconstruct.txt'\n",
    "\n",
    "data = load_data_halu_eval(halu_eval_file_path)\n",
    "\n",
    "with open(reconstruct_prompt_file_path, 'r', encoding='utf-8') as f:\n",
    "    reconstruct_prompt_template = f.read()\n",
    "\n",
    "print(f\"Length of HaluEval data: {len(data)}\\n\")\n",
    "print(f\"Reconstruct prompt: \\n{reconstruct_prompt_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2be2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 300/300 [04:16<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "result = []\n",
    "\n",
    "for idx, sample in enumerate(tqdm(data, desc=\"Processing data\")):\n",
    "    answer = sample['answer']\n",
    "    reconstruct_prompt = reconstruct_prompt_template.replace('{answer}', answer)\n",
    "    response = client.generate_response(reconstruct_prompt, n=5, temeprature=1)\n",
    "    response_cleaned = [question.strip('\"') for question in response]\n",
    "    result.append(\n",
    "        {\n",
    "            \"id\": sample['id'],\n",
    "            \"knowledge\": sample[\"knowledge\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"answer\": sample[\"answer\"],\n",
    "            \"reconstruct_questions\": response_cleaned,\n",
    "            \"is_hallucinated\": sample['is_hallucinated']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fdc8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(output_halu_eval_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
