{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0957794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of data before processed: 9997\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data_path = \"../../data/halu_eval/evaluation/qa/qa_gpt-3.5-turbo_result.json\"\n",
    "\n",
    "with open(data_path, 'r') as handle:\n",
    "    data = [json.loads(line) for line in handle]\n",
    "\n",
    "data = [item for item in data if 'judgement' in list(item.keys()) and 'ground_truth' in list(item.keys())]\n",
    "print(f\"Lenght of data before processed: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data with answer longer than 20: 5585\n",
      "Length of factual qas: 3702\n",
      "Length of hallucinated qas: 1883\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "answer_min_size = 50\n",
    "\n",
    "data = [item for item in data if len(item['answer']) > answer_min_size]\n",
    "print(f\"Length of data with answer longer than {answer_min_size}: {len(data)}\")\n",
    "factual_qas = [item for item in data if item['judgement'] == 'No']\n",
    "print(f\"Length of factual qas: {len(factual_qas)}\")\n",
    "hallucinated_qas = [item for item in data if item['judgement'] == 'Yes']\n",
    "print(f\"Length of hallucinated qas: {len(hallucinated_qas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece48cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucinated Size: 200\n",
      "Factual Size: 100\n",
      "Samples with judgement 'No': 300\n",
      "Samples with judgement 'Yes': 300\n",
      "Ground truth distribution: Counter({'Yes': 450, 'No': 150})\n",
      "Judgement distribution: Counter({'No': 300, 'Yes': 300})\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "sample_size = 300\n",
    "hallucinated_ratio = 2/3\n",
    "hallucinated_size = int(sample_size * hallucinated_ratio)\n",
    "factual_size = sample_size - hallucinated_size\n",
    "print(f\"Hallucinated Size: {hallucinated_size}\")\n",
    "print(f\"Factual Size: {factual_size}\")\n",
    "\n",
    "def sample_with_ground_truth(items, total_samples, ground_truth_ratio=0.5):\n",
    "    try:\n",
    "        gt_no = [item for item in items if item['ground_truth'] == 'No']\n",
    "        gt_yes = [item for item in items if item['ground_truth'] == 'Yes']\n",
    "\n",
    "        ground_truth_yes_size = int(total_samples * ground_truth_ratio)\n",
    "\n",
    "        sampled_no = random.sample(gt_no, ground_truth_yes_size)\n",
    "        sampled_yes = random.sample(gt_yes, sample_size - ground_truth_yes_size)\n",
    "        return sampled_no + sampled_yes\n",
    "    except:\n",
    "        return random.sample(items, total_samples)\n",
    "\n",
    "sampled_factual_qas = sample_with_ground_truth(factual_qas, hallucinated_size)\n",
    "sampled_hallucinated_qas = sample_with_ground_truth(hallucinated_qas, factual_size)\n",
    "combined_samples = sampled_factual_qas + sampled_hallucinated_qas\n",
    "random.shuffle(combined_samples)\n",
    "ground_truth_counts = Counter(item['ground_truth'] for item in combined_samples)\n",
    "judgement_counts = Counter(item['judgement'] for item in combined_samples)\n",
    "\n",
    "print(f\"Samples with judgement 'No': {len(sampled_factual_qas)}\")\n",
    "print(f\"Samples with judgement 'Yes': {len(sampled_hallucinated_qas)}\")\n",
    "print(f\"Ground truth distribution: {ground_truth_counts}\")\n",
    "print(f\"Judgement distribution: {judgement_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6096e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for idx, sample in enumerate(combined_samples):\n",
    "    result.append(\n",
    "        {\n",
    "            \"id\": idx,\n",
    "            \"knowledge\": sample[\"knowledge\"],\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"answer\": sample[\"answer\"],\n",
    "            \"ground_truth\": sample[\"ground_truth\"],\n",
    "            \"is_hallucinated\": 0 if sample['judgement'] == 'Yes' else 1\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2be82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../../data/interrogate_llm_zeroshot/halu_eval_long_answer.json\"\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
